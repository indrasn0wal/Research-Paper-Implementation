{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXryXbjOYE3I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block"
      ],
      "metadata": {
        "id": "8B7p-j7QoX0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, d_model:int, vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "  def forward():\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "gHMQj97zZYBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model:int, seq_len:int,dropout:float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    #Create a matrix (seq_len,d_model)\n",
        "    pe = torch.zeros(self.seq_len, self.d_model)\n",
        "    #Create a vector of (seq_len,1)\n",
        "    position = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
        "    #Denominator part\n",
        "    division = torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))\n",
        "    pe[:,0::2] = torch.sin(position * division)\n",
        "    pe[:,1::2] = torch.cos(position * division)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe',pe)\n",
        "  def forward(self,x):\n",
        "    x = x + (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "lLvoWIIzDUtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,eps:float=10**-6)-> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1)) #Multiplied\n",
        "    self.beta = nn.Parameter(torch.zeros(1)) # Added\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1,keepdim=True)\n",
        "    return self.alpha*(x-mean)/torch.sqrt(var+self.eps) + self.bias"
      ],
      "metadata": {
        "id": "zIQslYhSLZBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self,d_model:int,d_ff:int,dropout:float):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model,d_ff) #W1 and B1\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff,d_model) #W2 and B2\n",
        "  def forward(self,x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "SwG-If1gxYLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model:int,h:int,dropout:float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    assert d_model%h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model//h\n",
        "    self.w_q = nn.Linear(d_model,d_model)\n",
        "    self.w_k = nn.Linear(d_model,d_model)\n",
        "    self.w_v = nn.Linear(d_model,d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model,d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query,key,value,mask,dropout:nn.Dropout):\n",
        "    #query shape: (batch,h,seq_len,d_k)\n",
        "    d_k = query.shape[-1]\n",
        "    #shape: (batch,h,seq_len,d_k) * (batch,h,d_k,seq_len) -> (batch,h,seq_len,seq_len)\n",
        "    attention_scores = (query @ key.transpose(-2,-1))/math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask==0,-1e9)\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "    #(attention_scores @ value) -> (batch,h,seq_len,d_k)\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self,q,k,v,mask):\n",
        "    #shape:(batch,seq_len,d_model) -> (batch,seq_len,d_model)\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "    #shape: (batch,seq_len,d_model) -> (batch,seq_len,h,d_k) -> (batch,h,seq_len,d_k)\n",
        "    query = query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "    key = key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0],value.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "    #shape: x: (batch,h,seq_len,d_k)\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query,key,value,mask,self.dropout)\n",
        "    #shape: (batch,h,seq_len,d_k) -> (batch,seq_len,h,d_k) -> concat -> (batch,seq_len,d_model)\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h*self.d_k)\n",
        "    return self.w_o(x)"
      ],
      "metadata": {
        "id": "aeznXO3tMBQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self,dropout:float) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNorm()\n",
        "  def forward(self,x,sublayer):\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "GEGT1WfQ3ETO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,self_attention_block:MultiHeadAttention,feed_forward_block:FeedForwardBlock,dropout:float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.dropout = dropout\n",
        "    self.residual1 = ResidualConnection(dropout)\n",
        "    self.residual2 = ResidualConnection(dropout)\n",
        "  def forward(self, x, mask):\n",
        "    # First residual connection around self-attention\n",
        "    x = self.residual1(x, lambda x: self.self_attention_block(x, x, x, mask))\n",
        "\n",
        "    # Second residual connection around feed-forward\n",
        "    x = self.residual2(x, self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1D_d1r281Em9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,layers:nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNorm()\n",
        "  def forward(self,x,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "pKCu1-oKdUP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Block\n"
      ],
      "metadata": {
        "id": "d6TFt0N5oa0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,self_attention_block:MultiHeadAttention,cross_attention_block:MultiHeadAttention,feed_forward_block:FeedForwardBlock,dropout:float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.dropout = dropout\n",
        "    self.residual1 = ResidualConnection(dropout)\n",
        "    self.residual2 = ResidualConnection(dropout)\n",
        "    self.residual3 = ResidualConnection(dropout)\n",
        "  def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
        "    x = self.residual1(x,lambda x: self.self_attention_block(x,x,x,tgt_mask))\n",
        "    x = self.residual2(x,lambda x: self.cross_attention_block(x,encoder_output,encoder_output,src_mask))\n",
        "    x = self.residual3(x,self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZbYhA-e2oWft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,layers:nn.ModuleList)-> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNorm()\n",
        "  def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
        "    for layer in layers:\n",
        "      x = layer(x,encoder_output,src_mask,tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "VFSejSPz4-GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self,d_model:int,vocab_size:int) -> None:\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model,vocab_size)\n",
        "  def forward(self,x):\n",
        "    # (batch,seq_len,d_model) -> (batch,seq_len,vocab_size)\n",
        "    return torch.log_softmax(self.proj(x),dim=-1)"
      ],
      "metadata": {
        "id": "HZhIhgHupXS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block"
      ],
      "metadata": {
        "id": "C-nRJ-i8qEQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,encoder: Encoder,decoder:Decoder,src_embedding: InputEmbedding, tgt_embedding: InputEmbedding,src_pos: PositionalEncoding,tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embedding = src_embedding\n",
        "    self.tgt_embedding = tgt_embedding\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "  def encode(self,src,src_mask):\n",
        "    src = self.src_embedding(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src,src_mask)\n",
        "  def decode(self,encoder_output,src_mask,tgt,tgt_mask):\n",
        "    tgt = self.tgt_embedding(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n",
        "  def project(self,x):\n",
        "    return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "PTh8f-9_qB1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The main Function"
      ],
      "metadata": {
        "id": "mef_RJ1Dn8XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size:int,tgt_vocab_size:int,src_seq_len:int,tgt_seq_len:int,d_model:int=512,N:int=6,h:int=8,dropout:float=0.1,d_ff:int=2048) -> Transformer:\n",
        "  #Input Embedding\n",
        "  src_embedding = InputEmbedding(d_model,src_vocab_size)\n",
        "  tgt_embeding = InputEmbedding(d_model,tgt_vocab_size)\n",
        "  #Positional Embedding\n",
        "  src_pos = PositionalEncoding(d_model,src_seq_len,dropout)\n",
        "  tgt_pos = PositionalEncoding(d_model,tgt_seq_len,dropout)\n",
        "  #Encoder block\n",
        "  encoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiHeadAttention(d_model,h,dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model,d_ff,dropout)\n",
        "    encoder_blocks.append(EncoderBlock(\n",
        "        encoder_self_attention_block,\n",
        "        feed_forward_block,\n",
        "        dropout))\n",
        "  #Decoder blocks\n",
        "  decoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiHeadAttention(d_mode,h,dropout)\n",
        "    decoder_cross_attention_block = MultiHeadAttention(d_model,h,dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model,d_ff,dropout)\n",
        "    decoder_blocks.append(DecoderBlock(\n",
        "        decoder_self_attention_block,\n",
        "        decoder_cross_attention_block,\n",
        "        feed_forward_block,\n",
        "        dropout))\n",
        "  #Create Encoder and Decoder\n",
        "  encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "  decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "  #Create the projection layer\n",
        "  projection_layer = ProjectionLayer(d_model,tgt_vocab_size)\n",
        "  #Create the transformer\n",
        "  transformer = Transformer(encoder,decoder,src_embedding, tgt_embedding,src_pos,tgt_pos, projection_layer)\n",
        "  #Initialize the parameters\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ],
      "metadata": {
        "id": "lwkr6EdWk3qu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}